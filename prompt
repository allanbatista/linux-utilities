#!/usr/bin/env python3
"""
CLI para concatenar conteúdo de arquivos e enviar para um LLM provider.
Agora com suporte a ChatGPT (OpenAI) **sem dependências extras** (usa somente `requests`).

Configuração do modelo/provedor via `~/.prompt/config.json` (opcional).

Exemplo de `~/.prompt/config.json`:
{
  "provider": "chatgpt",                 // "gemini" ou "chatgpt" (default: gemini)
  "model": "gpt-4o-mini",               // default global (pode ser sobrescrito por provider)
  "providers": {
    "chatgpt": {
      "model": "gpt-4o-mini",           // nome do modelo OpenAI
      "api_key_env": "OPENAI_API_KEY",  // nome da variável de ambiente
      "api_base": "https://api.openai.com/v1"
    },
    "gemini": {
      "model": "gemini-1.5-flash-latest",
      "api_key_env": "GEMINI_API_KEY"
    }
  },
  "request": { "timeout_seconds": 300 }
}

Novidade:
- Flag `--set-default-model <modelo>` para **persistir** o modelo default em `~/.prompt/config.json` (chave top-level `model`).
  Se usada isoladamente (sem `-p` e sem caminhos), apenas atualiza o config e encerra.
"""
import argparse
import os
import pathlib
import json
import requests
import pyperclip
import datetime
import sys
from typing import Optional, Tuple, Dict, Any

# =========================
# Utilitários e Persistência
# =========================

def load_config() -> Dict[str, Any]:
    """Carrega ~/.prompt/config.json se existir; caso contrário, retorna {}."""
    try:
        cfg_path = pathlib.Path.home() / ".prompt" / "config.json"
        if cfg_path.exists():
            with open(cfg_path, "r", encoding="utf-8") as f:
                return json.load(f)
    except Exception as e:
        print(f"Aviso: não foi possível ler ~/.prompt/config.json: {e}")
    return {}


def persist_default_model(new_model: str) -> bool:
    """
    Atualiza o modelo default em ~/.prompt/config.json (chave top-level "model"),
    preservando os demais campos. Cria o arquivo se não existir.
    """
    try:
        cfg_dir = pathlib.Path.home() / ".prompt"
        cfg_dir.mkdir(parents=True, exist_ok=True)
        cfg_path = cfg_dir / "config.json"

        config: Dict[str, Any] = {}
        if cfg_path.exists():
            try:
                with open(cfg_path, "r", encoding="utf-8") as f:
                    config = json.load(f) or {}
            except json.JSONDecodeError:
                config = {}
        else:
            config = {}

        config["model"] = new_model

        with open(cfg_path, "w", encoding="utf-8") as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        return True
    except Exception as e:
        print(f"Erro ao persistir modelo default: {e}")
        return False


# =========================
# Providers
# =========================

def build_specialist_prefix(specialist: Optional[str]) -> str:
    specialist_prompts = {
        'dev': 'Aja como um programador sênior especialista em desenvolvimento de software, com mais de 20 anos de experiência. Suas respostas devem ser claras, eficientes, bem-estruturadas e seguir as melhores práticas do mercado. Pense passo a passo.',
        'rm': 'Aja como um analista de Retail Media sênior, especialista em estratégias de publicidade digital para e-commerce e marketplaces. Seu conhecimento abrange plataformas como Amazon Ads, Mercado Ads e Criteo. Suas respostas devem ser analíticas, estratégicas e baseadas em dados.'
    }
    return specialist_prompts.get(specialist or "", "")


def send_to_gemini(prompt: str, context: str, lang: str, specialist: Optional[str], model_name: str, timeout_s: int, api_key_env: str = "GEMINI_API_KEY") -> Optional[Dict[str, Any]]:
    """
    Envia o prompt e o contexto para a API REST do Gemini (Google).
    """
    api_key = os.getenv(api_key_env)
    if not api_key:
        print(f"Erro: A variável de ambiente {api_key_env} não está definida.")
        return None

    prompt_parts = []
    specialist_prefix = build_specialist_prefix(specialist)
    if specialist_prefix:
        prompt_parts.append(specialist_prefix)
    prompt_parts.append(f"{prompt}\n\n--- CONTEXTO DOS ARQUIVOS ---\n\n{context}")
    prompt_parts.append(f"--- INSTRUÇÃO DE SAÍDA ---\nResponda estritamente na linguagem: {lang}.")

    full_prompt = "\n\n".join(prompt_parts)
    api_version = 'v1beta'
    url = f"https://generativelanguage.googleapis.com/{api_version}/models/{model_name}:generateContent?key={api_key}"
    payload = {"contents": [{"parts": [{"text": full_prompt}]}]}
    headers = {"Content-Type": "application/json"}

    try:
        print(f"Enviando requisição para o Gemini {model_name} (API {api_version})...")
        response = requests.post(url, headers=headers, data=json.dumps(payload), timeout=timeout_s)
        response.raise_for_status()
        response_data = response.json()
        text_response = response_data['candidates'][0]['content']['parts'][0]['text']
        usage_metadata = response_data.get('usageMetadata', {})
        prompt_tokens = usage_metadata.get('promptTokenCount', 'N/A')
        response_tokens = usage_metadata.get('candidatesTokenCount', 'N/A')
        
        return {
            "provider": "gemini",
            "text": text_response,
            "prompt_tokens": prompt_tokens,
            "response_tokens": response_tokens,
            "full_prompt": full_prompt,
            "model": model_name,
        }

    except requests.exceptions.RequestException as e:
        print(f"Ocorreu um erro de rede ou HTTP ao chamar a API do Gemini: {e}")
        if getattr(e, 'response', None) is not None:
            try:
                print(f"Detalhes do erro: {e.response.text}")
            except Exception:
                pass
        return None
    except (KeyError, IndexError) as e:
        print(f"Erro ao extrair o conteúdo da resposta da API (Gemini): {e}")
        try:
            print("Estrutura da resposta recebida:", response.json())
        except Exception:
            pass
        return None
    except Exception as e:
        print(f"Um erro inesperado ocorreu (Gemini): {e}")
        return None


def send_to_chatgpt(prompt: str, context: str, lang: str, specialist: Optional[str], model_name: str, timeout_s: int, api_key_env: str = "OPENAI_API_KEY", api_base: str = "https://api.openai.com/v1") -> Optional[Dict[str, Any]]:
    """
    Envia o prompt e o contexto para a API REST do OpenAI Chat Completions (ChatGPT).
    Usa somente `requests`.
    """
    api_key = os.getenv(api_key_env)
    if not api_key:
        print(f"Erro: A variável de ambiente {api_key_env} não está definida.")
        return None

    # Mensagens no formato Chat Completions
    sys_parts = []
    specialist_prefix = build_specialist_prefix(specialist)
    if specialist_prefix:
        sys_parts.append(specialist_prefix)
    sys_parts.append(f"Responda estritamente na linguagem: {lang}.")
    system_msg = {"role": "system", "content": "\n\n".join(sys_parts) or "Você é um assistente útil."}

    user_content = f"{prompt}\n\n--- CONTEXTO DOS ARQUIVOS ---\n\n{context}"
    user_msg = {"role": "user", "content": user_content}

    url = f"{api_base.rstrip('/')}/chat/completions"
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
    }
    payload = {
        "model": model_name,
        "messages": [system_msg, user_msg],
    }

    try:
        print(f"Enviando requisição para ChatGPT {model_name} (Chat Completions)...")
        response = requests.post(url, headers=headers, data=json.dumps(payload), timeout=timeout_s)
        response.raise_for_status()
        data = response.json()
        text_response = data['choices'][0]['message']['content']
        usage = data.get('usage', {})
        prompt_tokens = usage.get('prompt_tokens', 'N/A')
        response_tokens = usage.get('completion_tokens', 'N/A')

        full_prompt = (system_msg["content"] + "\n\n" + user_content)
        return {
            "provider": "chatgpt",
            "text": text_response,
            "prompt_tokens": prompt_tokens,
            "response_tokens": response_tokens,
            "full_prompt": full_prompt,
            "model": model_name,
        }

    except requests.exceptions.RequestException as e:
        print(f"Ocorreu um erro de rede ou HTTP ao chamar a API do OpenAI: {e}")
        if getattr(e, 'response', None) is not None:
            try:
                print(f"Detalhes do erro: {e.response.text}")
            except Exception:
                pass
        return None
    except (KeyError, IndexError) as e:
        print(f"Erro ao extrair o conteúdo da resposta da API (OpenAI): {e}")
        try:
            print("Estrutura da resposta recebida:", response.json())
        except Exception:
            pass
        return None
    except Exception as e:
        print(f"Um erro inesperado ocorreu (OpenAI): {e}")
        return None


# =========================
# Processamento de Arquivos
# =========================

def process_file(file_path: pathlib.Path, path_format: str, max_tokens_doc: int) -> Tuple[str, int, int]:
    """
    Lê o conteúdo de um arquivo, formata o cabeçalho e trunca se necessário com base em tokens.

    Args:
        file_path: O caminho do arquivo a ser processado.
        path_format: Como o caminho deve ser formatado ('full', 'relative', 'name_only').
        max_tokens_doc: O número máximo de tokens estimados para este arquivo.

    Returns:
        Uma tupla contendo o conteúdo formatado, a contagem de palavras e os tokens estimados.
    """
    try:
        display_path = ""
        if path_format == 'name_only':
            display_path = file_path.name
        elif path_format == 'relative':
            display_path = os.path.relpath(file_path.resolve(), pathlib.Path.cwd())
        else: # 'full'
            display_path = str(file_path.resolve())

        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
        
        original_tokens = len(content) // 4
        warning_message = ""

        if original_tokens > max_tokens_doc:
            max_chars = max_tokens_doc * 4
            content = content[:max_chars]
            warning_message = (
                f"// warning_content_truncated=\"true\" "
                f"original_token_count=\"{original_tokens}\" "
                f"new_token_count=\"{max_tokens_doc}\"\n"
            )
            print(f"  -> Aviso: O arquivo '{display_path}' foi truncado para ~{max_tokens_doc} tokens.")

        word_count = len(content.split())
        estimated_tokens = len(content) // 4
        formatted_content = f"// filename=\"{display_path}\"\n{warning_message}{content}\n"
        
        return formatted_content, word_count, estimated_tokens
    except Exception as e:
        error_message = f"// error_processing_file=\"{file_path.resolve()}\"\n// Error: {e}\n"
        return error_message, 0, 0


# =========================
# Configuração Efetiva
# =========================

def resolve_settings(args, config: Dict[str, Any]) -> Dict[str, Any]:
    """Resolve provider/model/timeout/api_base/api_key_env a partir de args + config."""
    provider = args.provider or config.get("provider") or "gemini"

    providers = config.get("providers", {})
    p_cfg = providers.get(provider, {}) if isinstance(providers, dict) else {}

    # Model precedence: CLI > provider-specific > top-level > hardcoded default
    if args.model:
        model = args.model
    else:
        model = p_cfg.get("model") or config.get("model")
        if not model:
            model = "gemini-1.5-flash-latest" if provider == "gemini" else "gpt-4o-mini"

    # API key env var name
    api_key_env = p_cfg.get("api_key_env")
    if not api_key_env:
        api_key_env = "GEMINI_API_KEY" if provider == "gemini" else "OPENAI_API_KEY"

    # API base (usado apenas para chatgpt)
    api_base = p_cfg.get("api_base") or "https://api.openai.com/v1"

    # Timeout
    timeout_s = int(config.get("request", {}).get("timeout_seconds", 300))

    return {
        "provider": provider,
        "model": model,
        "api_key_env": api_key_env,
        "api_base": api_base,
        "timeout_s": timeout_s,
    }


# =========================
# Main
# =========================

def main():
    """Função principal que orquestra a execução do script."""
    ALLOWED_EXTENSIONS = {
        '.txt', '.py', '.rb', '.rs', '.html', '.css', '.js', '.ts', '.cs',
        '.sh', '.md', '.c', '.cpp', '.hpp', '.h', '.json', '.yml', '.yaml',
        '.jsonl', '.xml', '.scss'
    }

    parser = argparse.ArgumentParser(
        description=(
            "Concatena o conteúdo de arquivos com extensões permitidas e "
            "opcionalmente envia para a API do Gemini ou ChatGPT."
        ),
        formatter_class=argparse.RawTextHelpFormatter
    )
    parser.add_argument(
        "paths",
        metavar="PATH",
        type=pathlib.Path,
        nargs='*',
        help="Uma lista de arquivos e/ou diretórios para processar."
    )
    parser.add_argument(
        "-p", "--prompt",
        type=str,
        help="Um prompt opcional para enviar à API junto com o conteúdo dos arquivos."
    )
    parser.add_argument(
        '--lang',
        type=str,
        default='pt-br',
        help='Linguagem de output desejada. Padrão: pt-br'
    )
    parser.add_argument(
        '-n', '--max-tokens',
        type=int,
        default=900_000,
        help='Tamanho máximo em tokens estimados para o contexto total. Padrão: 900000'
    )
    parser.add_argument(
        '-nn', '--max-tokens-doc',
        type=int,
        default=250_000,
        help='Tamanho máximo em tokens estimados para cada arquivo individual. Padrão: 250000'
    )
    parser.add_argument(
        '-s', '--specialist',
        type=str,
        choices=['dev', 'rm'],
        help=(
            "Define uma persona especialista:\n"
            "'dev' para Programador Sênior\n"
            "'rm'  para Analista de Retail Media Sênior."
        )
    )
    parser.add_argument(
        '--provider',
        type=str,
        choices=['gemini', 'chatgpt'],
        help='Define o provedor a ser usado (gemini/chatgpt). Se omitido, usa ~/.prompt/config.json ou gemini.'
    )
    parser.add_argument(
        '--model',
        type=str,
        help='Nome do modelo a ser usado. Se omitido, usa ~/.prompt/config.json ou padrão do provedor.'
    )
    parser.add_argument(
        '--set-default-model',
        type=str,
        help='Define e persiste o modelo padrão (top-level "model") em ~/.prompt/config.json e encerra, se nenhum outro argumento for passado.'
    )

    path_options = parser.add_mutually_exclusive_group()
    path_options.add_argument(
        "--relative-paths",
        action="store_true",
        help="Exibe caminhos relativos em vez de caminhos absolutos."
    )
    path_options.add_argument(
        "--filename-only",
        action="store_true",
        help="Exibe apenas o nome do arquivo em vez do caminho completo."
    )
    
    # Se nenhum argumento for passado, exibe a ajuda
    if len(sys.argv) == 1:
        parser.print_help(sys.stderr)
        sys.exit(1)

    args = parser.parse_args()

    # Atualiza modelo default, se solicitado
    if args.set_default_model:
        if persist_default_model(args.set_default_model):
            print(f"✅ Modelo default atualizado para: {args.set_default_model} em ~/.prompt/config.json")
        else:
            print("Erro ao atualizar o modelo default.")
        # Se apenas setou o default e não forneceu prompt nem caminhos, encerra.
        if not args.prompt and len(args.paths) == 0:
            return

    # Carrega configurações
    config = load_config()
    settings = resolve_settings(args, config)

    path_format_option = 'full'
    if args.relative_paths:
        path_format_option = 'relative'
    elif args.filename_only:
        path_format_option = 'name_only'

    all_files_content = []
    total_word_count = 0
    total_estimated_tokens = 0
    files_processed_count = 0
    files_error_count = 0
    files_skipped_count = 0

    for path_arg in args.paths:
        if not path_arg.exists():
            print(f"Aviso: O caminho '{path_arg}' não existe. Pulando.")
            continue

        if path_arg.is_file():
            if path_arg.suffix in ALLOWED_EXTENSIONS:
                content, word_count, estimated_tokens = process_file(path_arg, path_format_option, args.max_tokens_doc)
                print(f"Processando arquivo: {path_arg.resolve()} ({word_count} palavras, ~{estimated_tokens} tokens)")
                if content.startswith("// error_processing_file"):
                    files_error_count += 1
                else:
                    files_processed_count += 1
                    total_word_count += word_count
                    total_estimated_tokens += estimated_tokens
                all_files_content.append(content)
            else:
                print(f"Aviso: Arquivo com extensão não permitida '{path_arg.suffix}' foi ignorado: {path_arg}")
                files_skipped_count += 1
        
        elif path_arg.is_dir():
            print(f"Processando diretório: {path_arg.resolve()}")
            for child_path in path_arg.rglob('*'):
                if child_path.is_file():
                    if child_path.suffix in ALLOWED_EXTENSIONS:
                        content, word_count, estimated_tokens = process_file(child_path, path_format_option, args.max_tokens_doc)
                        print(f"  -> Processando: {child_path.relative_to(path_arg)} ({word_count} palavras, ~{estimated_tokens} tokens)")
                        if content.startswith("// error_processing_file"):
                            files_error_count += 1
                        else:
                            files_processed_count += 1
                            total_word_count += word_count
                            total_estimated_tokens += estimated_tokens
                        all_files_content.append(content)
                    else:
                        files_skipped_count += 1
        else:
            print(f"Aviso: O caminho '{path_arg}' não é um arquivo nem um diretório. Pulando.")

    final_text = "".join(all_files_content)

    # Caso nenhum arquivo tenha sido processado
    if not final_text and not args.prompt:
        print("\nNenhum arquivo válido foi encontrado ou processado.")
        if files_skipped_count > 0:
            print(f"{files_skipped_count} arquivo(s) foram ignorados devido à extensão não permitida.")
        return

    original_total_tokens = len(final_text) // 4
    if args.max_tokens and original_total_tokens > args.max_tokens:
        print(f"\nAviso: O contexto final com ~{original_total_tokens} tokens excedeu o limite de {args.max_tokens}. Truncando...")
        max_chars = args.max_tokens * 4
        final_text = final_text[:max_chars]
        print(f"Novo total de tokens estimados no contexto: ~{len(final_text) // 4}")

    # Realiza a chamada ao provider caso exista prompt
    if args.prompt:
        prov = settings["provider"]
        model = settings["model"]
        timeout_s = settings["timeout_s"]
        api_key_env = settings["api_key_env"]

        if prov == "chatgpt":
            api_base = settings["api_base"]
            result = send_to_chatgpt(args.prompt, final_text, args.lang, args.specialist, model, timeout_s, api_key_env=api_key_env, api_base=api_base)
        else:
            result = send_to_gemini(args.prompt, final_text, args.lang, args.specialist, model, timeout_s, api_key_env=api_key_env)

        if result:
            response_text = result['text']
            
            print("\n--- INFORMAÇÕES DA REQUISIÇÃO ---")
            print(f"Provider Utilizado: {result['provider']}")
            print(f"Modelo Utilizado: {result['model']}")
            print(f"Arquivos Processados: {files_processed_count} ({total_word_count} palavras, ~{total_estimated_tokens} tokens) | Erros: {files_error_count} | Ignorados: {files_skipped_count}")
            print(f"Tokens Enviados (API): {result['prompt_tokens']}")
            print(f"Tokens Recebidos (API): {result['response_tokens']}")
            print("---------------------------------")
            
            print("\n--- RESPOSTA DO MODELO ---\n")
            print(response_text)
            print("\n--------------------------\n")
            
            try:
                pyperclip.copy(response_text)
                print("✅ Resposta copiada para a área de transferência!")
            except pyperclip.PyperclipException as e:
                print(f"Erro: Não foi possível copiar para a área de transferência. {e}")

            save_to_history(result['full_prompt'], response_text)
        return

    # Se não houver prompt, mas houver conteúdo de arquivo, copie para o clipboard
    if final_text:
        try:
            pyperclip.copy(final_text)
            print(f"\nProcessado(s) {files_processed_count} arquivo(s) com sucesso ({total_word_count} palavras, ~{total_estimated_tokens} tokens no total).")
            if files_skipped_count > 0:
                 print(f"{files_skipped_count} arquivo(s) foram ignorados devido à extensão não permitida.")
            if files_error_count > 0:
                print(f"Encontrados erros em {files_error_count} arquivo(s).")
            print("✅ O conteúdo combinado foi copiado para a sua área de transferência!")
        except pyperclip.PyperclipException as e:
            print(f"\nErro: Não foi possível copiar para a área de transferência. {e}")
            print("\nAqui está a saída combinada:\n")
            print("--------------------------------------------------")
            print(final_text)
            print("--------------------------------------------------")


if __name__ == "__main__":
    main()
